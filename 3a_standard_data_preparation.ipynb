{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3a_standard_data_preparation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KYQXdo5scWW_","colab_type":"text"},"source":["# Creating Standard Training Data\n","\n","In this notebook we will be doing the following :\n","  1. Build & perform basic text cleaning operations/pipeline on the documents\n","  2. Convert the data in training data format. i.e. _label_tag1 _label_tag2\n","  3. Split the dataset into : train,dev and test sets  \n","  4.  Divide the dataset into Groups/Classes\n","\n","    a. Dividing the entire Dataset(~1M) into 14 groups/class.\n","\n","    b. Check the Label Distribution of Labels in each 14 groups/class.\n","\n","\n","  5. Create Corpus & Label Dictionary : Flair Corpus\n","\n","**HOT TIP** : *Save them as pickle for easy rendering for experiments*\n","\n"]},{"cell_type":"code","metadata":{"id":"LILjzCaXcguK","colab_type":"code","colab":{}},"source":["# First let's check what has Google given us ! Thank you Google for the GPU\n","\n","!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGKB2JRvYnpx","colab_type":"code","outputId":"d4d5c666-02fc-431f-aa66-69cfda67a299","executionInfo":{"status":"ok","timestamp":1576826756844,"user_tz":-330,"elapsed":45906,"user":{"displayName":"Amit Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBt42SjR9APoAR-qVJSOkBpHyvvmw8o2Dw4mJpS=s64","userId":"09342634691867414754"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Let's mount our G-Drive. Hey !! Because for GPU you now give your data to Google \n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LtRJwG4oLgWg","colab_type":"code","outputId":"86fc36ea-0f8f-4e22-af3f-153a63883812","executionInfo":{"status":"ok","timestamp":1576826803156,"user_tz":-330,"elapsed":34181,"user":{"displayName":"Amit Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBt42SjR9APoAR-qVJSOkBpHyvvmw8o2Dw4mJpS=s64","userId":"09342634691867414754"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Install necessary packages and restart the environment\n","\n","! pip install tiny-tokenizer\n","! pip install  flair"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting flair\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/22/8fc8e5978ec05b710216735ca47415700e83f304dec7e4281d61cefb6831/flair-0.4.4-py3-none-any.whl (193kB)\n","\u001b[K     |████████████████████████████████| 194kB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.0)\n","Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n","Collecting transformers>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/08/4a6768ca1a7a4fa37e5ee08077c5d02b8d83876bd36caa5fc24d98992ac2/transformers-2.2.2-py3-none-any.whl (387kB)\n","\u001b[K     |████████████████████████████████| 389kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.1.2)\n","Collecting langdetect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n","\u001b[K     |████████████████████████████████| 1.0MB 6.5MB/s \n","\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.6)\n","Collecting bpemb>=0.2.9\n","  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n","Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n","Collecting sqlitedict>=1.6.0\n","  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n","Collecting mpld3==0.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n","\u001b[K     |████████████████████████████████| 798kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from flair) (3.10.0)\n","Collecting ipython==7.6.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/2c/c7d44277b599df35af734d8f4142d501192fdb7aef5d04daf882d7eccfbc/ipython-7.6.1-py3-none-any.whl (774kB)\n","\u001b[K     |████████████████████████████████| 778kB 10.9MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.3.1)\n","Collecting tiny-tokenizer[all]\n","  Downloading https://files.pythonhosted.org/packages/8c/ee/08078f68165a7465f028f3505e6a749b50f6f5c229bd272a863ab07acdc2/tiny_tokenizer-3.0.1.tar.gz\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.2)\n","Collecting deprecated>=1.2.4\n","  Downloading https://files.pythonhosted.org/packages/f6/89/62912e01f3cede11edcc0abf81298e3439d9c06c8dce644369380ed13f6d/Deprecated-1.2.7-py2.py3-none-any.whl\n","Collecting segtok>=1.5.7\n","  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.9)\n","Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 13.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 16.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.17.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (1.10.40)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.0.0->flair) (2.21.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.6.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->flair) (1.12.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.9.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (42.0.2)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.3.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (8.0.2)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (2.1.3)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.3.3)\n","Collecting prompt-toolkit<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/61/2dfea88583d5454e3a64f9308a686071d58d59a55db638268a6413e1eb6d/prompt_toolkit-2.0.10-py3-none-any.whl (340kB)\n","\u001b[K     |████████████████████████████████| 348kB 19.3MB/s \n","\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.7.5)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.7.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (4.4.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.1.0)\n","Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython==7.6.1->flair) (0.15.1)\n","Collecting natto-py\n","  Downloading https://files.pythonhosted.org/packages/2f/a0/eaac1ed66c02823a2423a21de863da53a5268ce77582d91d1edb45a403dc/natto-py-0.9.0.tar.gz\n","Collecting kytea\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/bc/702d01a96d5d094bd9f3c2eb1d12153daf8edf7bf5d78b9a2dae1202df07/kytea-0.1.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 20.7MB/s \n","\u001b[?25hCollecting SudachiPy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/c9/40bfb291a7995ad218451ef97083432f998b822e3ecbd9f586f593d2cfb6/SudachiPy-0.4.2-py3-none-any.whl (73kB)\n","\u001b[K     |████████████████████████████████| 81kB 9.9MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->flair) (4.3.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (1.13.40)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.0.0->flair) (3.0.4)\n","Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.6.0)\n","Requirement already satisfied: parso>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.5.2)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from natto-py->tiny-tokenizer[all]->flair) (1.13.2)\n","Collecting dartsclone~=0.6.0\n","  Downloading https://files.pythonhosted.org/packages/7d/4d/45acbe9d0795d8ceef0fee1f9ac2dcbf27dca3a0578a023fcdc3fef6fd89/dartsclone-0.6.tar.gz\n","Requirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy->tiny-tokenizer[all]->flair) (2.1.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->flair) (0.46)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers>=2.0.0->flair) (0.15.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->natto-py->tiny-tokenizer[all]->flair) (2.19)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.6.0->SudachiPy->tiny-tokenizer[all]->flair) (0.29.14)\n","Building wheels for collected packages: langdetect, sqlitedict, mpld3, tiny-tokenizer, segtok, sacremoses, natto-py, dartsclone\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=10334ec6e1f5ef05ef3434701a3676e8318febb2063b241ec6ae33d2c8313995\n","  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=573bcbda8732f8ce230e2f0a3969374d55540a1ec975f24d0dfaa7b1d89ec1d0\n","  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=b5facd964de7d6c4a13056173332cb23b36576ed310130ca253d93fca4909b09\n","  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n","  Building wheel for tiny-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tiny-tokenizer: filename=tiny_tokenizer-3.0.1-cp36-none-any.whl size=9444 sha256=69b0345797448f5def420b373b1888077bac3f51211bd8e2d41a435858bdfe38\n","  Stored in directory: /root/.cache/pip/wheels/76/04/72/d04956c4b03e3b03e5e095c06cbabc9bfb6f1bec02288eacdb\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segtok: filename=segtok-1.5.7-cp36-none-any.whl size=23258 sha256=b1c94dc40640207f0b3a33317983ecc60c29fdfde6bd0fabd919aa977c7d542f\n","  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=b34548190fc58d9151602d6528f090d42585c88e61d9dcbb4a2392d750241e95\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","  Building wheel for natto-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for natto-py: filename=natto_py-0.9.0-cp36-none-any.whl size=45075 sha256=b78676a36717821a5340e0a907c22bf037d165ab44543d03f0c41671d952b115\n","  Stored in directory: /root/.cache/pip/wheels/21/98/3a/ebfb1636e18698b3f47d8caa3f90fc3a91f1ea58430616018f\n","  Building wheel for dartsclone (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dartsclone: filename=dartsclone-0.6-cp36-cp36m-linux_x86_64.whl size=413261 sha256=a573fe99127510b7ac12a5bffe6b9b06eaf6b18a5cd9db45593705f826d87666\n","  Stored in directory: /root/.cache/pip/wheels/be/cd/70/fe43307bf7398243155108f4f5a258ef336923d65ec4af93cd\n","Successfully built langdetect sqlitedict mpld3 tiny-tokenizer segtok sacremoses natto-py dartsclone\n","\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.6.1 which is incompatible.\u001b[0m\n","Installing collected packages: sentencepiece, sacremoses, transformers, langdetect, bpemb, sqlitedict, mpld3, prompt-toolkit, ipython, natto-py, kytea, dartsclone, SudachiPy, tiny-tokenizer, deprecated, segtok, flair\n","  Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","Successfully installed SudachiPy-0.4.2 bpemb-0.3.0 dartsclone-0.6 deprecated-1.2.7 flair-0.4.4 ipython-7.6.1 kytea-0.1.4 langdetect-1.0.7 mpld3-0.3 natto-py-0.9.0 prompt-toolkit-2.0.10 sacremoses-0.0.35 segtok-1.5.7 sentencepiece-0.1.85 sqlitedict-1.6.0 tiny-tokenizer-3.0.1 transformers-2.2.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","prompt_toolkit"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"9sZizSqp62NH","colab_type":"code","colab":{}},"source":["# Let's import our packages !\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import html\n","import re\n","from bs4 import BeautifulSoup\n","import re\n","from sklearn.model_selection import train_test_split\n","# import flair\n","import pickle\n","from torch.optim.adam import Adam\n","\n","# Making Corpus\n","\n","from flair.data import Corpus\n","from flair.datasets import ClassificationCorpus\n","from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n","from flair.models import TextClassifier\n","from flair.trainers import ModelTrainer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrYBqDyFeM9v","colab_type":"code","colab":{}},"source":["## Mentioning where is our data located on G-Drive. Make sure to rectify your path\n","\n","path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/'\n","tag_group = '/content/drive/My Drive/ICDMAI_Tutorial/stack-overflow-tag-network/stack_network_nodes.csv'\n","data ='filtered_data/question_tag_text_mapping.pkl'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxJhWn1PeNSG","colab_type":"code","outputId":"6fcee4cd-16c5-4f85-ce8c-e8658a589719","executionInfo":{"status":"ok","timestamp":1576730278946,"user_tz":-330,"elapsed":25009,"user":{"displayName":"Amit Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBt42SjR9APoAR-qVJSOkBpHyvvmw8o2Dw4mJpS=s64","userId":"09342634691867414754"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Let's see the main Data-Set\n","\n","question_tag = pd.read_pickle(path+data)\n","question_tag.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>OwnerUserId</th>\n","      <th>CreationDate</th>\n","      <th>ClosedDate</th>\n","      <th>Score</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>CreationMonth</th>\n","      <th>CreationYear</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>120</td>\n","      <td>83.0</td>\n","      <td>2008-08-01 15:50:08</td>\n","      <td>NaN</td>\n","      <td>21</td>\n","      <td>ASP.NET Site Maps</td>\n","      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n","      <td>8</td>\n","      <td>2008</td>\n","      <td>[asp.net, sql]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>260</td>\n","      <td>91.0</td>\n","      <td>2008-08-01 23:22:08</td>\n","      <td>NaN</td>\n","      <td>49</td>\n","      <td>Adding scripting functionality to .NET applica...</td>\n","      <td>&lt;p&gt;I have a little game written in C#. It uses...</td>\n","      <td>8</td>\n","      <td>2008</td>\n","      <td>[c#, .net]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>330</td>\n","      <td>63.0</td>\n","      <td>2008-08-02 02:51:36</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>Should I use nested classes in this case?</td>\n","      <td>&lt;p&gt;I am working on a collection of classes use...</td>\n","      <td>8</td>\n","      <td>2008</td>\n","      <td>[c++]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>470</td>\n","      <td>71.0</td>\n","      <td>2008-08-02 15:11:47</td>\n","      <td>2016-03-26T05:23:29Z</td>\n","      <td>13</td>\n","      <td>Homegrown consumption of web services</td>\n","      <td>&lt;p&gt;I've been writing a few web services for a ...</td>\n","      <td>8</td>\n","      <td>2008</td>\n","      <td>[web-services, .net]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>580</td>\n","      <td>91.0</td>\n","      <td>2008-08-02 23:30:59</td>\n","      <td>NaN</td>\n","      <td>21</td>\n","      <td>Deploying SQL Server Databases from Test to Live</td>\n","      <td>&lt;p&gt;I wonder how you guys manage deployment of ...</td>\n","      <td>8</td>\n","      <td>2008</td>\n","      <td>[sql-server]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Id  OwnerUserId  ... CreationYear                   Tag\n","0  120         83.0  ...         2008        [asp.net, sql]\n","1  260         91.0  ...         2008            [c#, .net]\n","2  330         63.0  ...         2008                 [c++]\n","3  470         71.0  ...         2008  [web-services, .net]\n","4  580         91.0  ...         2008          [sql-server]\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"QpNjj0P8dddt","colab_type":"code","outputId":"68301f6e-c540-4c43-aae8-21d326a64573","executionInfo":{"status":"ok","timestamp":1576773626786,"user_tz":-330,"elapsed":3261,"user":{"displayName":"Amit Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBt42SjR9APoAR-qVJSOkBpHyvvmw8o2Dw4mJpS=s64","userId":"09342634691867414754"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Let's see the Groups/Classes\n","\n","tag_group = pd.read_csv(tag_group)\n","tag_group.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>group</th>\n","      <th>nodesize</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>html</td>\n","      <td>6</td>\n","      <td>272.45</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>css</td>\n","      <td>6</td>\n","      <td>341.17</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hibernate</td>\n","      <td>8</td>\n","      <td>29.83</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>spring</td>\n","      <td>8</td>\n","      <td>52.84</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ruby</td>\n","      <td>3</td>\n","      <td>70.14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        name  group  nodesize\n","0       html      6    272.45\n","1        css      6    341.17\n","2  hibernate      8     29.83\n","3     spring      8     52.84\n","4       ruby      3     70.14"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"CHS8mZmWc-h6","colab_type":"text"},"source":["## 1. Text Pre-processing Pipeline\n","\n","Every try-except block can be written as a different modular function which can be invoked from preprocess_text() function. This serves as a pipeline of the series of text-cleaning that you might require for your dataset."]},{"cell_type":"code","metadata":{"id":"X1qOCcSAQ-Og","colab_type":"code","colab":{}},"source":["clean = re.compile('<.*?>')\n","\n","def preprocess_text(text) :\n","  try :\n","    # soup = BeautifulSoup(text, \"html.parser\")\n","    # text = soup.get_text()\n","    text=  re.sub(clean, '', text)\n","    text = html.unescape(text)\n","  except :\n","    print(\"Error in HTML Processing ...\")\n","    print(text)\n","    text = text\n","  try :\n","    # remove extra newlines (often might be present in really noisy text)\n","    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n","  except :\n","    print(\"Error in removing extra lines ...\")\n","    print(text)\n","    text = text\n","\n","  try :\n","    # remove extra whitespace\n","    text = re.sub(' +', ' ', text)\n","    text = text.strip()\n","  except :\n","    print(\"Error in extra whitespace removal ...\")\n","    print(text)\n","    text = text\n","\n","  return text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q0BxD9_GdENx","colab_type":"text"},"source":["## 2. Create Training Data Format\n","\n","Here we iterate the dataset dataframe and create the format acceptable to Flair. This is a standard format for few other Text Classification models/frameworks by Facebook.\n","\n","***Format***  : ____label ____**tag1** ____label ____**tag2** **text**\n","\n","Here the text Document has to be in a single line which was handled in the preprocess_text() method."]},{"cell_type":"code","metadata":{"id":"Ds6SmNxXjSxM","colab_type":"code","colab":{}},"source":["def create_training_format(question_tag) :\n","\n","  print(\"Preparing training data format ...\")\n","  # training_df = pd.DataFrame(\"columns\")\n","  labels = list()\n","  texts = list()\n","  for index in tqdm(question_tag.index) :\n","    tags = question_tag.loc[index,'Tag']\n","    text_label = ''\n","    for tag in tags :\n","      label = '__label__'+tag\n","      text_label = text_label + ' ' + label\n","    \n","    text_label = text_label.strip()\n","    # text =  html.unescape(question_tag.loc[index,'Body'])\n","    text =  question_tag.loc[index,'Title'].strip() + '. ' + question_tag.loc[index,'Body'].strip()\n","\n","    # if len(text.split()) < 5 :\n","    #   continue \n","\n","    labels.append(text_label)\n","    texts.append(text)\n","\n","\n","  df = pd.DataFrame(list(zip(labels[:], texts[:])), columns =['label', 'text']) \n","  # df.head()\n","  print(\"Cleaning Text ....\")\n","  df['text'] = df['text'].apply(preprocess_text)\n","  print(\"Cleaned Data Size : {}\".format(df.shape))\n","\n","  return df\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"utCSLYPHdKgm","colab_type":"text"},"source":["## 3. Create Training Splits\n","\n","Here we create standard random splits of the dataset to :\n","  1. training set : 90 % data\n","  2. dev/validation set :  10 % data\n","  3. test set : 10 % data\n","\n","#### TO DO : Experiments :\n","  1. Stratified Sampling\n","  2. Does a  70-15-15 split or 90-5-5 split make any difference when you ahve 1M records ?"]},{"cell_type":"code","metadata":{"id":"bbJT8_O5dK-S","colab_type":"code","colab":{}},"source":["def create_splits(df,path,group_id = ''):\n","\n","  print(\"Splitting Training Data ... \")\n","  train_df , test_df = train_test_split(df,random_state=42,test_size=0.30)\n","  dev_df ,test_df = train_test_split(test_df,random_state=42,test_size=0.5)\n","  print(\"Training Dataset : {}\".format(train_df.shape[0]))\n","  print(\"Validation Dataset : {}\".format(dev_df.shape[0]))\n","  print(\"Test Dataset : {}\".format(test_df.shape[0]))\n","\n","  print(\"Path  : {} \".format(path+'training_data/group/'+ str(group_id) + '/train.txt'))\n","  train_df.to_csv(path+'training_data/group/'+ str(group_id) + '/train.txt',sep='\\t',index=False,header=False)\n","  dev_df.to_csv(path+'training_data/group/'+ str(group_id) + '/dev.txt',sep='\\t',index=False,header=False)\n","  test_df.to_csv(path+'training_data/group/'+ str(group_id) + '/test.txt',sep='\\t',index=False,header=False)\n","\n","  return train_df,dev_df,test_df\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIjUDgjTdj5J","colab_type":"text"},"source":["## 4. Divide the dataset into Groups/Classes\n","\n","Here we iterate over the entire dataset to create group level datasets in the following steps :\n","  1. Iterate over the groups and read the full-dataset eveytime\n","  2. Get all the tags in the group from the **tag_group** lookup\n","  3. Iterate over training examples and see if the labels fall in the same group\n","  4. Remove training examples which don't belong to the group\n","  5. Create the training data format of the remaining dataset\n","  6. Split & Save the dataset\n","\n","  ### TO DO :\n","    1. Make a single corpus for the entire dataset.\n"]},{"cell_type":"code","metadata":{"id":"PUQl8oD_eSn6","colab_type":"code","outputId":"298bdd01-43c9-4d97-f564-41f2e6a65d92","executionInfo":{"status":"ok","timestamp":1576774437303,"user_tz":-330,"elapsed":753469,"user":{"displayName":"Amit Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBt42SjR9APoAR-qVJSOkBpHyvvmw8o2Dw4mJpS=s64","userId":"09342634691867414754"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for grp_id in range(1,15):\n","  \n","  ## 1. Iterate over the groups and read the full-dataset eveytime\n","  print(\"=================================================================\")\n","  print(\"Group ID being Processed : {}\".format(grp_id))\n","  print(\"=================================================================\")\n","  print(\"Reading Pickle File ...\")\n","\n","  question_tag = pd.read_pickle(path+data)\n","\n","  ## 2. Get all the tags in the group from the **tag_group** lookup  \n","\n","  group =  tag_group[tag_group.group == grp_id]\n","  labels = list(set(group['name']))\n","\n","  ## 3. Iterate over training examples and see if the labels fall in the same group\n","  for index in tqdm(question_tag.index):\n","    tags = question_tag.loc[index,'Tag']\n","    group_tags = list()\n","    for tag in tags :\n","      if tag in labels :\n","        group_tags.append(tag)\n","    question_tag.at[index,'Tag'] =  group_tags\n","  print(\"Before Removal of Blank Data : {} \".format(question_tag.shape))\n","\n","   ## 4. Remove training examples which don't belong to the group\n","  question_tag = question_tag[question_tag['Tag'].map(lambda d: len(d)) > 0]\n","  print(\"Final Data for Group ID  : {} is {}\".format(grp_id,question_tag.shape))\n","\n","  ## 5. Create the training data format of the remaining dataset\n","  training_data_format = create_training_format(question_tag)\n","\n","  ## 6. Split & Save the dataset\n","  train_df,dev_df,test_df = create_splits(training_data_format,path,group_id=grp_id)\n","\n","  print(\"=================================================================\")\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1873/1051992 [00:00<00:56, 18726.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 2 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 27773.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["  2%|▏         | 3787/196254 [00:00<00:05, 37869.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 2 is (196254, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 196254/196254 [00:04<00:00, 40483.37it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (196254, 2)\n","Splitting Training Data ... \n","Training Dataset : 137377\n","Validation Dataset : 29438\n","Test Dataset : 29439\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/2/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1911/1051992 [00:00<00:54, 19107.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 3 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 27904.62it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["  6%|▋         | 4126/65453 [00:00<00:01, 41259.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 3 is (65453, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 65453/65453 [00:01<00:00, 40072.83it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (65453, 2)\n","Splitting Training Data ... \n","Training Dataset : 45817\n","Validation Dataset : 9818\n","Test Dataset : 9818\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/3/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2233/1051992 [00:00<00:47, 22326.24it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 4 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:38<00:00, 27463.32it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["  2%|▏         | 4160/169599 [00:00<00:03, 41593.20it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 4 is (169599, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 169599/169599 [00:04<00:00, 39921.11it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (169599, 2)\n","Splitting Training Data ... \n","Training Dataset : 118719\n","Validation Dataset : 25440\n","Test Dataset : 25440\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/4/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2035/1051992 [00:00<00:51, 20346.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 5 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 27976.69it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["  8%|▊         | 4180/54356 [00:00<00:01, 41793.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 5 is (54356, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 54356/54356 [00:01<00:00, 40572.70it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (54356, 2)\n","Splitting Training Data ... \n","Training Dataset : 38049\n","Validation Dataset : 8153\n","Test Dataset : 8154\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/5/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1911/1051992 [00:00<00:54, 19109.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 6 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 27710.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["  1%|          | 3883/356875 [00:00<00:09, 38826.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 6 is (356875, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 356875/356875 [00:08<00:00, 39733.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (356875, 2)\n","Splitting Training Data ... \n","Training Dataset : 249812\n","Validation Dataset : 53531\n","Test Dataset : 53532\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/6/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1845/1051992 [00:00<00:56, 18437.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 7 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:36<00:00, 28654.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3460/3460 [00:00<00:00, 35486.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 7 is (3460, 10)\n","Preparing training data format ...\n","Cleaning Text ....\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaned Data Size : (3460, 2)\n","Splitting Training Data ... \n","Training Dataset : 2422\n","Validation Dataset : 519\n","Test Dataset : 519\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/7/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2146/1051992 [00:00<00:48, 21455.72it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 8 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 28387.56it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["  3%|▎         | 4045/143543 [00:00<00:03, 40447.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 8 is (143543, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 143543/143543 [00:03<00:00, 40256.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (143543, 2)\n","Splitting Training Data ... \n","Training Dataset : 100480\n","Validation Dataset : 21531\n","Test Dataset : 21532\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/8/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1604/1051992 [00:00<01:05, 16039.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 9 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:38<00:00, 27611.78it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":[" 46%|████▋     | 4103/8850 [00:00<00:00, 41026.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 9 is (8850, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 8850/8850 [00:00<00:00, 39520.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (8850, 2)\n","Splitting Training Data ... \n","Training Dataset : 6195\n","Validation Dataset : 1327\n","Test Dataset : 1328\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/9/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2316/1051992 [00:00<00:45, 23158.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 10 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:36<00:00, 28819.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":[" 33%|███▎      | 4231/12979 [00:00<00:00, 42303.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 10 is (12979, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 12979/12979 [00:00<00:00, 40248.98it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (12979, 2)\n","Splitting Training Data ... \n","Training Dataset : 9085\n","Validation Dataset : 1947\n","Test Dataset : 1947\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/10/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 2249/1051992 [00:00<00:46, 22484.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 11 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 28330.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 5703/5703 [00:00<00:00, 40276.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 11 is (5703, 10)\n","Preparing training data format ...\n","Cleaning Text ....\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaned Data Size : (5703, 2)\n","Splitting Training Data ... \n","Training Dataset : 3992\n","Validation Dataset : 855\n","Test Dataset : 856\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/11/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1889/1051992 [00:00<00:55, 18886.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 12 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 27885.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 570/570 [00:00<00:00, 36411.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 12 is (570, 10)\n","Preparing training data format ...\n","Cleaning Text ....\n","Cleaned Data Size : (570, 2)\n","Splitting Training Data ... \n","Training Dataset : 399\n","Validation Dataset : 85\n","Test Dataset : 86\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/12/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  0%|          | 2043/1051992 [00:00<00:51, 20429.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 13 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:37<00:00, 27844.14it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":[" 22%|██▏       | 4421/19933 [00:00<00:00, 44203.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 13 is (19933, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 19933/19933 [00:00<00:00, 39517.02it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (19933, 2)\n","Splitting Training Data ... \n","Training Dataset : 13953\n","Validation Dataset : 2990\n","Test Dataset : 2990\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/13/train.txt \n","Reading Pickle File ...\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 1807/1051992 [00:00<00:58, 18064.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Current Group ID : 14 \n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1051992/1051992 [00:38<00:00, 27594.32it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Before Removal of Blank Data : (1051992, 10) \n"],"name":"stdout"},{"output_type":"stream","text":[" 27%|██▋       | 3885/14291 [00:00<00:00, 38846.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Final Data for Group ID  : 14 is (14291, 10)\n","Preparing training data format ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 14291/14291 [00:00<00:00, 36915.84it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Cleaning Text ....\n","Cleaned Data Size : (14291, 2)\n","Splitting Training Data ... \n","Training Dataset : 10003\n","Validation Dataset : 2144\n","Test Dataset : 2144\n","Path  : /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/14/train.txt \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VxJTJlRAfCJO","colab_type":"text"},"source":["## 5. Create Corpus & Label Dictionary : Flair Corpus\n","\n","For all the training splits created above for each group, we will be creating a corpus & vocabulary to train a different model."]},{"cell_type":"code","metadata":{"id":"mE1VjpMKdHyY","colab_type":"code","outputId":"76fd0e47-0c83-4461-c731-56a56f183b43","executionInfo":{"status":"ok","timestamp":1576829852167,"user_tz":-330,"elapsed":2464425,"user":{"displayName":"Amit Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBt42SjR9APoAR-qVJSOkBpHyvvmw8o2Dw4mJpS=s64","userId":"09342634691867414754"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/'\n","\n","for grp in range(1,15):\n","  \n","  print(\"=================================================================\")\n","  print(\"Group ID being Processed : {}\".format(grp))\n","  print(\"=================================================================\")\n","  \n","  # this is the folder in which train, test and dev files reside\n","  data_folder =path+str(grp)+'/'\n","  print(data_folder)\n","\n","  print(\"Creating Corpus ...\")\n","  # load corpus containing training, test and dev data\n","  corpus: Corpus = ClassificationCorpus(data_folder,\n","                                        test_file='test.txt',\n","                                        dev_file='dev.txt',\n","                                        train_file='train.txt')\n","\n","  # 2. create the label dictionary\n","  label_dict = corpus.make_label_dictionary()\n","\n","  print(\"Obtaining Corpus Statisitics...\")\n","  stats  = corpus.obtain_statistics()\n","  json_stats = json.loads(stats)\n","\n","  print(json_stats)\n","  \n","  with open(data_folder+'corpus_statistics.json', 'w') as f:\n","    json.dump(json_stats, f)\n","\n","  print(\"Creating Dumps ... \")\n","  with open(data_folder+'classification_corpus.pkl',mode='wb') as f :\n","    pickle.dump(corpus,f)\n","\n","  with open(data_folder + 'classification_corpus_label_dict.pkl',mode='wb') as f:\n","    pickle.dump(label_dict,f)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/1/\n","Creating Corpus ...\n","2019-12-20 07:36:28,399 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/1\n","2019-12-20 07:36:28,402 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/1/train.txt\n","2019-12-20 07:36:28,404 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/1/dev.txt\n","2019-12-20 07:36:28,406 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/1/test.txt\n","2019-12-20 07:36:37,861 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 114121/114121 [05:25<00:00, 350.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:42:03,422 [b'python', b'django', b'r', b'c', b'c++', b'matlab', b'qt', b'embedded', b'machine-learning', b'flask']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/2/\n","Creating Corpus ...\n","2019-12-20 07:42:03,438 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/2\n","2019-12-20 07:42:03,439 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/2/train.txt\n","2019-12-20 07:42:03,440 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/2/dev.txt\n","2019-12-20 07:42:03,441 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/2/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:42:12,136 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 137377/137377 [06:14<00:00, 366.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:48:26,966 [b'sql-server', b'sql', b'wpf', b'.net', b'c#', b'visual-studio', b'wcf', b'unity3d', b'asp.net', b'vb.net', b'asp.net-web-api', b'oracle', b'entity-framework', b'azure', b'linq', b'xamarin', b'plsql']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/3/\n","Creating Corpus ...\n","2019-12-20 07:48:27,001 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/3\n","2019-12-20 07:48:27,001 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/3/train.txt\n","2019-12-20 07:48:27,003 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/3/dev.txt\n","2019-12-20 07:48:27,004 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/3/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:48:33,096 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 45817/45817 [02:04<00:00, 366.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:50:38,081 [b'node.js', b'ruby-on-rails', b'mongodb', b'ruby', b'express', b'elasticsearch', b'reactjs', b'redux', b'postgresql', b'redis', b'react-native']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/4/\n","Creating Corpus ...\n","2019-12-20 07:50:38,092 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/4\n","2019-12-20 07:50:38,093 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/4/train.txt\n","2019-12-20 07:50:38,094 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/4/dev.txt\n","2019-12-20 07:50:38,096 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/4/test.txt\n","2019-12-20 07:50:45,712 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 118719/118719 [05:51<00:00, 337.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:56:37,485 [b'iphone', b'ios', b'android', b'objective-c', b'xcode', b'osx', b'swift', b'android-studio']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/5/\n","Creating Corpus ...\n","2019-12-20 07:56:37,503 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/5\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:56:37,504 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/5/train.txt\n","2019-12-20 07:56:37,506 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/5/dev.txt\n","2019-12-20 07:56:37,508 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/5/test.txt\n","2019-12-20 07:56:42,386 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 38049/38049 [01:39<00:00, 380.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 07:58:22,489 [b'unix', b'git', b'windows', b'apache', b'shell', b'bash', b'linux', b'github', b'ubuntu', b'powershell', b'nginx']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/6/\n","Creating Corpus ...\n","2019-12-20 07:58:22,508 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/6\n","2019-12-20 07:58:22,509 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/6/train.txt\n","2019-12-20 07:58:22,511 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/6/dev.txt\n","2019-12-20 07:58:22,512 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/6/test.txt\n","2019-12-20 07:58:34,837 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 249812/249812 [11:16<00:00, 369.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:09:50,995 [b'php', b'mysql', b'json', b'html', b'javascript', b'jquery', b'angularjs', b'twitter-bootstrap-3', b'css', b'twitter-bootstrap', b'ajax', b'xml', b'laravel', b'wordpress', b'less', b'codeigniter', b'drupal', b'html5', b'vue.js', b'sass', b'ionic-framework', b'photoshop']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/7/\n","Creating Corpus ...\n","2019-12-20 08:09:51,024 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/7\n","2019-12-20 08:09:51,025 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/7/train.txt\n","2019-12-20 08:09:51,027 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/7/dev.txt\n","2019-12-20 08:09:51,028 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/7/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:09:53,125 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2422/2422 [00:06<00:00, 368.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:09:59,820 [b'typescript', b'angular2']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/8/\n","Creating Corpus ...\n","2019-12-20 08:09:59,840 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/8\n","2019-12-20 08:09:59,841 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/8/train.txt\n","2019-12-20 08:09:59,844 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/8/dev.txt\n","2019-12-20 08:09:59,848 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/8/test.txt\n","2019-12-20 08:10:06,881 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100480/100480 [05:05<00:00, 329.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:15:12,303 [b'java', b'spring', b'rest', b'web-services', b'hibernate', b'eclipse', b'api', b'jsp', b'maven', b'java-ee', b'spring-boot', b'spring-mvc']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/9/\n","Creating Corpus ...\n","2019-12-20 08:15:12,323 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/9\n","2019-12-20 08:15:12,325 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/9/train.txt\n","2019-12-20 08:15:12,327 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/9/dev.txt\n","2019-12-20 08:15:12,333 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/9/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:15:15,482 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 6195/6195 [00:17<00:00, 355.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:15:33,039 [b'jenkins', b'amazon-web-services', b'docker', b'cloud', b'go', b'devops']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/10/\n","Creating Corpus ...\n","2019-12-20 08:15:33,052 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/10\n","2019-12-20 08:15:33,053 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/10/train.txt\n","2019-12-20 08:15:33,055 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/10/dev.txt\n","2019-12-20 08:15:33,056 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/10/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:15:35,809 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 9085/9085 [00:26<00:00, 341.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:02,513 [b'hadoop', b'scala', b'haskell', b'apache-spark']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/11/\n","Creating Corpus ...\n","2019-12-20 08:16:02,528 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/11\n","2019-12-20 08:16:02,529 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/11/train.txt\n","2019-12-20 08:16:02,531 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/11/dev.txt\n","2019-12-20 08:16:02,533 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/11/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:05,235 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3992/3992 [00:09<00:00, 411.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:15,068 [b'selenium', b'testing']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/12/\n","Creating Corpus ...\n","2019-12-20 08:16:15,078 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/12\n","2019-12-20 08:16:15,079 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/12/train.txt\n","2019-12-20 08:16:15,081 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/12/dev.txt\n","2019-12-20 08:16:15,082 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/12/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:17,769 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 399/399 [00:01<00:00, 318.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:19,153 [b'agile', b'tdd']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/13/\n","Creating Corpus ...\n","2019-12-20 08:16:19,166 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/13\n","2019-12-20 08:16:19,167 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/13/train.txt\n","2019-12-20 08:16:19,170 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/13/dev.txt\n","2019-12-20 08:16:19,171 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/13/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:25,533 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 13953/13953 [00:29<00:00, 471.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:16:55,248 [b'regex', b'perl']\n","Creating Dumps ... \n","/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/14/\n","Creating Corpus ...\n","2019-12-20 08:16:55,261 Reading data from /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/14\n","2019-12-20 08:16:55,262 Train: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/14/train.txt\n","2019-12-20 08:16:55,264 Dev: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/14/dev.txt\n","2019-12-20 08:16:55,265 Test: /content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/14/test.txt\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:17:01,750 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10003/10003 [00:29<00:00, 335.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["2019-12-20 08:17:31,656 [b'excel', b'vba', b'excel-vba']\n","Creating Dumps ... \n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]}]}