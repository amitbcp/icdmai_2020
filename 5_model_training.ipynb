{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cisVHJmIhbx1","colab_type":"text"},"source":["# Text Classification Model\n","\n","In this notebook we will train & explore different Text Classifiers to solve our problem.\n","\n","  1. Train a single classifier variations.\n","  2. Train multiple classifiers on unbalanced dataset.\n","  3. Train multiple classifiers on balanced dataset."]},{"cell_type":"code","metadata":{"id":"wWbxHQ8Ji5Be","colab_type":"code","colab":{}},"source":["# First let's check what has Google given us ! Thank you Google for the GPU\n","!nvidia-smi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvrv7VkPi572","colab_type":"code","colab":{}},"source":["# Let's mount our G-Drive. Hey !! Because for GPU you now give your data to Google \n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"93PiLfdmi7z7","colab_type":"code","colab":{}},"source":["# Install necessary packages and restart the environment\n","\n","! pip install tiny-tokenizer\n","! pip install  flair\n","! pip install -U tensorflow-gpu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16x7OOFJjIJX","colab_type":"code","colab":{}},"source":["# Let's import our packages !\n","\n","import pandas as pd\n","from tqdm import tqdm\n","import html\n","import re\n","from bs4 import BeautifulSoup\n","import re\n","from sklearn.model_selection import train_test_split\n","import flair\n","import pickle\n","from torch.optim.adam import Adam\n","\n","from flair.data import Corpus\n","from flair.datasets import ClassificationCorpus\n","from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings,FastTextEmbeddings\n","from flair.models import TextClassifier\n","from flair.trainers import ModelTrainer\n","from flair.samplers import ImbalancedClassificationDatasetSampler\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKwcewlCjSsy","colab_type":"code","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ItWgf2pyjwQy","colab_type":"text"},"source":["# 1. Train a Single Classifier\n","\n","The first approach, would be to load the entire dataset(~1M) and train a single classifier powerful & mighty enough to perform great.\n","\n","Challenges :\n","  * Super slow to experiment with different architecture\n","  * Super slow to train a model\n","  * Super slow for Hyper-parameter tuning\n","  * Highly Skewed Dataset\n","\n","\n","Experiments :\n","  * Try stacking multiple embeddings at #3\n","  * Try custom embeddings at #3\n","  * Try different RNN cell at #4\n","  * Try different hidden size at #4\n","  * All hyperparameters are tunable for extensive experimenting at #7"]},{"cell_type":"code","metadata":{"id":"-2aapmebk03R","colab_type":"code","colab":{}},"source":["path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/'\n","corpus_path = path+'training_data/classification_corpus.pkl'\n","label_dict_path = path+'training_data/classification_corpus_label_dict.pkl'\n","\n","\n","# 1. Reading Corpus File : which we prepared before-hand\n","with open(corpus_path, mode='rb') as f:\n","  corpus = pickle.load(f)\n","\n","# 2. Reading Corpus Dictionary : which we computed & saved\n","with open(label_dict_path, mode='rb') as f:\n","  label_dict = pickle.load(f)\n","\n","# 3. make a list of word embeddings \n","word_embeddings = [\n","  WordEmbeddings('glove'),\n","\n","  # comment in flair embeddings for state-of-the-art results\n","  # FlairEmbeddings('news-forward'),\n","  # FlairEmbeddings('news-backward'),\n","]\n","\n","# 4. initialize document embedding by passing list of word embeddings\n","## Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n","document_embeddings = DocumentRNNEmbeddings(\n","  word_embeddings,\n","  hidden_size=128,\n","  reproject_words=True,\n","  reproject_words_dimension=256,\n",")\n","\n","# 5. create the text classifier\n","classifier = TextClassifier(document_embeddings,\n","                            label_dictionary=label_dict,\n","                            multi_label=True)\n","\n","# 6. initialize the text classifier trainer\n","trainer = ModelTrainer(classifier,\n","                        corpus,\n","                        optimizer=Adam,\n","                        use_tensorboard=True)\n","\n","# 7. start the training\n","model_path = path + '/model/full_model'\n","trainer.train(model_path,\n","              learning_rate=0.06,\n","              mini_batch_size=32,\n","              anneal_factor=0.5,\n","              patience=5,\n","              max_epochs=2,\n","              checkpoint=True,\n","              embeddings_storage_mode='gpu',\n","              num_workers=12)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itvLVvPC2VB3","colab_type":"text"},"source":["# 2. Train Multiple Classifiers on unbalanced Dataset\n","\n","A single classifier fails to capture and fit the data, performing poorly on all the metrices. This led us to divide the problem into sub-groups and FOCUS more on individual group than Training a Titan Model for everything.\n","\n","In this section, we would train multiple classifiers on the different groups that we create.\n","\n","### Experiments \n","\n","  1. We chose a couple of representative groups\n","  2. We try different architectures & embeddings\n","  3. We train only 2 epochs for quick results\n","  4. We manually try couple of Hyper-parameter tuning based on our Hypothesis\n","\n","\n","### Things to try & Build Hypothesis on:\n","  1. GRU/LSTM cells\n","  2. Number of RNN Layers \n","  3. Hidden Units / Time steps / Sequence Length\n","  4. Embeddings\n","  5. Batch Size\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"oHuiL2795lE8","colab_type":"code","colab":{}},"source":["for grp_id in [3,11] :\n","  print(\"================================================================================================\")\n","  print(\"Group ID : {}\".format(grp_id))\n","  print(\"================================================================================================\")\n","\n","  path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/'\n","  base_path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/group/' + str(grp_id) + '/'\n","\n","  corpus_path =  path + str(grp_id) + '/classification_corpus.pkl'\n","  label_dict_path = path + str(grp_id) + '/classification_corpus_label_dict.pkl'\n","\n","  # 1. Reading Corpus File : which we prepared before-hand\n","  with open(corpus_path, mode='rb') as f:\n","    corpus = pickle.load(f)\n","\n","  # 2. Reading Corpus Dictionary : which we computed & saved\n","  with open(label_dict_path, mode='rb') as f:\n","    label_dict = pickle.load(f)\n","\n","  # 3. make a list of word embeddings \n","  word_embeddings = [ \n","                     WordEmbeddings('/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/model_300/gensim_model'), # Custom Word Embedding \n","                    ## comment in different embeddings for state-of-the-art results\n","                     \n","                    #  WordEmbeddings('glove'),                 \n","                    # FlairEmbeddings('news-forward'),\n","                    # FlairEmbeddings('news-backward'),\n","                     \n","    ]\n","\n","  # Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n","  document_embeddings = DocumentRNNEmbeddings(\n","    word_embeddings,\n","    hidden_size=256, # Build a hypothesis for different values\n","    rnn_layers = 2,  # Build a hypothesis for different values\n","    reproject_words=True, \n","    reproject_words_dimension=256\n","  )\n","\n","  classifier = TextClassifier(document_embeddings,\n","                            label_dictionary=label_dict,\n","                            multi_label=True)\n","\n","  # 6. initialize the text classifier trainer\n","  trainer = ModelTrainer(classifier,\n","                          corpus,\n","                          optimizer=Adam\n","                          )\n","\n","  # model_path = path + '/model_70_30/full_model_1'\n","\n","  trainer.train(base_path,\n","                learning_rate=0.1,\n","                mini_batch_size=128,\n","                anneal_factor=0.5,\n","                patience=5,\n","                max_epochs=2,\n","                checkpoint=True,\n","                )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LHNjIkkdQKp0","colab_type":"text"},"source":["Once with you experiments, you have finalised top 2-3 architecture and configurations, then use it to train the classifiers.\n","\n","**Note** : You should individually run these experiments for all the groups and train custom model for each of them. Here we will be using a vanila configuration for all the groups."]},{"cell_type":"code","metadata":{"id":"cbMcPewzQjrS","colab_type":"code","colab":{}},"source":["for grp_id in range(1,15) :\n","\n","  print(\"================================================================================================\")\n","  print(\"Group ID : {}\".format(grp_id))\n","  print(\"================================================================================================\")\n","\n","  path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/training_data/group/'\n","  base_path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/training_data/group/' + str(grp_id) + '/'\n","\n","  corpus_path =  path + str(grp_id) + '/classification_corpus.pkl'\n","  label_dict_path = path + str(grp_id) + '/classification_corpus_label_dict.pkl'\n","\n","  # 1. Reading Corpus File : which we prepared before-hand\n","  with open(corpus_path, mode='rb') as f:\n","    corpus = pickle.load(f)\n","\n","  # 2. Reading Corpus Dictionary : which we computed & saved\n","  with open(label_dict_path, mode='rb') as f:\n","    label_dict = pickle.load(f)\n","\n","  # 3. make a list of word embeddings \n","  word_embeddings = [ \n","                     WordEmbeddings('/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/model_300/gensim_model'), # Custom Word Embedding \n","                     \n","    ]\n","\n","  # Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n","  document_embeddings = DocumentRNNEmbeddings(\n","    word_embeddings,\n","    hidden_size=128, \n","    rnn_layers = 1,  \n","    reproject_words=True, \n","    reproject_words_dimension=256\n","  )\n","\n","  classifier = TextClassifier(document_embeddings,\n","                            label_dictionary=label_dict,\n","                            multi_label=True)\n","\n","  # 6. initialize the text classifier trainer\n","  trainer = ModelTrainer(classifier,\n","                          corpus,\n","                          optimizer=Adam\n","                          )\n","\n","\n","  trainer.train(base_path,\n","                learning_rate=0.1,\n","                mini_batch_size=128,\n","                anneal_factor=0.5,\n","                patience=5,\n","                max_epochs=10,\n","                checkpoint=True,\n","                )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmbE8U1yWhSM","colab_type":"text"},"source":["# 3. Train Multiple Classifiers on Balanced Dataset\n","\n","\n","Though now we see that there has been some improvement in the metrices with multiple classifiers in their respective groups validation/test datasets. it should be still be noted that those groups are fairly skewed and gives a hard time to the model to learn meaningful relations.\n","\n","Hence now we train the model with the normalised dataset that we prepared. \n","\n","**Note** : It should be noted that for every dataset/problem different ways of balancing/normalising the dataset works. Here clipping off worked well for us but penalising loss for less representative class did not.\n","\n","We run the same set of experiments & build a hypothesis.\n","\n","### Experiments \n","\n","  1. We chose a couple of representative groups\n","  2. We try different architectures & embeddings\n","  3. We train only 2 epochs for quick results\n","  4. We manually try couple of Hyper-parameter tuning based on our Hypothesis\n","\n","\n","### Things to try & Build Hypothesis on:\n","  1. GRU/LSTM cells\n","  2. Number of RNN Layers \n","  3. Hidden Units / Time steps / Sequence Length\n","  4. Embeddings\n","  5. Batch Size\n","\n"]},{"cell_type":"code","metadata":{"id":"kkMxNCwSY60t","colab_type":"code","colab":{}},"source":["for grp_id in [3,11] :\n","  print(\"================================================================================================\")\n","  print(\"Group ID : {}\".format(grp_id))\n","  print(\"================================================================================================\")\n","\n","  path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/normalised_training_data/group/'\n","  base_path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/normalised_training_data/group/' + str(grp_id) + '/'\n","\n","  corpus_path =  path + str(grp_id) + '/classification_corpus.pkl'\n","  label_dict_path = path + str(grp_id) + '/classification_corpus_label_dict.pkl'\n","\n","  # 1. Reading Corpus File : which we prepared before-hand\n","  with open(corpus_path, mode='rb') as f:\n","    corpus = pickle.load(f)\n","\n","  # 2. Reading Corpus Dictionary : which we computed & saved\n","  with open(label_dict_path, mode='rb') as f:\n","    label_dict = pickle.load(f)\n","\n","  # 3. make a list of word embeddings \n","  word_embeddings = [ \n","                     WordEmbeddings('/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/model_300/gensim_model'), # Custom Word Embedding \n","                    ## comment in different embeddings for state-of-the-art results\n","                     \n","                    #  WordEmbeddings('glove'),                 \n","                    # FlairEmbeddings('news-forward'),\n","                    # FlairEmbeddings('news-backward'),\n","                     \n","    ]\n","\n","  # Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n","  document_embeddings = DocumentRNNEmbeddings(\n","    word_embeddings,\n","    hidden_size=64, # Build a hypothesis for different values\n","    rnn_layers = 1,  # Build a hypothesis for different values\n","    bidirectional = True, # Trying changing the behaviour of the model\n","    reproject_words=True, \n","    reproject_words_dimension=256,\n","    dropout = 0 ,\n","    rnn_type = 'LSTM'\n","  )\n","\n","  classifier = TextClassifier(document_embeddings,\n","                            label_dictionary=label_dict,\n","                            multi_label_threshold = 0.3 , # Check with different Thresholds\n","                            multi_label=True)\n","\n","  # 6. initialize the text classifier trainer\n","  trainer = ModelTrainer(classifier,\n","                          corpus,\n","                          optimizer=Adam\n","                          )\n","\n","\n","  trainer.train(base_path,\n","                learning_rate=0.06,\n","                mini_batch_size=64,\n","                anneal_factor=0.5,\n","                patience=5,\n","                max_epochs=2,\n","                checkpoint=True,\n","                sampler=ImbalancedClassificationDatasetSampler # Check if puishing the mis-classification of less frequent labels heavily helps?\n","                )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5CX_0uJaGAN","colab_type":"text"},"source":["Once with you experiments, you have finalised top 2-3 architecture and configurations, then use it to train the classifiers.\n","\n","**Note** : You should individually run these experiments for all the groups and train custom model for each of them. Here we will be using a vanila configuration for all the groups.\n","\n","**Fun Fact** : We ran ~80 experiments for architecture for this small dataset itself to build the hypothesis for this Demonstrations"]},{"cell_type":"code","metadata":{"id":"bJhCZEYjaTyR","colab_type":"code","colab":{}},"source":["for grp_id in range(1,15) :\n","  print(\"================================================================================================\")\n","  print(\"Group ID : {}\".format(grp_id))\n","  print(\"================================================================================================\")\n","\n","  path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/normalised_training_data/group/'\n","  base_path = '/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/normalised_training_data/group/' + str(grp_id) + '/'\n","\n","  corpus_path =  path + str(grp_id) + '/classification_corpus.pkl'\n","  label_dict_path = path + str(grp_id) + '/classification_corpus_label_dict.pkl'\n","\n","  # 1. Reading Corpus File : which we prepared before-hand\n","  with open(corpus_path, mode='rb') as f:\n","    corpus = pickle.load(f)\n","\n","  # 2. Reading Corpus Dictionary : which we computed & saved\n","  with open(label_dict_path, mode='rb') as f:\n","    label_dict = pickle.load(f)\n","\n","  # 3. make a list of word embeddings \n","  word_embeddings = [ \n","                     WordEmbeddings('/content/drive/My Drive/ICDMAI_Tutorial/notebook/training_data/model_300/gensim_model'), # Custom Word Embedding \n","                     \n","    ]\n","\n","  # Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n","  document_embeddings = DocumentRNNEmbeddings(\n","    word_embeddings,\n","    hidden_size=64, # Build a hypothesis for different values\n","    rnn_layers = 1,  # Build a hypothesis for different values\n","    bidirectional = True, # Trying changing the behaviour of the model\n","    reproject_words=True, \n","    reproject_words_dimension=256,\n","    dropout = 0 ,\n","    rnn_type = 'LSTM'\n","  )\n","\n","  classifier = TextClassifier(document_embeddings,\n","                            label_dictionary=label_dict,\n","                            multi_label_threshold = 0.1 , # Check with different Thresholds\n","                            multi_label=True)\n","\n","  # 6. initialize the text classifier trainer\n","  trainer = ModelTrainer(classifier,\n","                          corpus,\n","                          optimizer=Adam\n","                          )\n","\n","\n","  trainer.train(base_path,\n","                learning_rate=0.03,\n","                mini_batch_size=16,\n","                anneal_factor=0.5,\n","                patience=5,\n","                max_epochs=10,\n","                checkpoint=True\n","                )"],"execution_count":0,"outputs":[]}]}